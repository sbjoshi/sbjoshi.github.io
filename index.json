[{"authors":["admin"],"categories":null,"content":"I am a Researcher working with a private company. All the information/views expressed here are my own and do not represent views of any of my past or current employer.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1687862761,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sbjoshi.github.io/author/saurabh-joshi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/saurabh-joshi/","section":"authors","summary":"I am a Researcher working with a private company. All the information/views expressed here are my own and do not represent views of any of my past or current employer.","tags":null,"title":"Saurabh Joshi","type":"authors"},{"authors":["Prasanth Chakka","Saurabh Joshi","Aniket Kate","Joshua Tobkin","David Yang"],"categories":null,"content":"","date":1686096e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687861575,"objectID":"bf836f35c33fd23f32fb04dcf9c7c1e3","permalink":"https://sbjoshi.github.io/publication/dora-icdcs2023/","publishdate":"2023-06-07T00:00:00Z","relpermalink":"/publication/dora-icdcs2023/","section":"publication","summary":"This paper provides a distributed Byzantine agreement with honest simple majority when State Machine Replication (SMR) is available as an ordering primitive.","tags":["Distributed Systems","Blockchain"],"title":"DORA: Distributed Agreement with Simple Majority","type":"publication"},{"authors":null,"categories":null,"content":"DORA DORA is a scaleable, transparent and efficient distributed oracle agreement protocole designed and developed at SupraOracles. Find more details here\n","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687862761,"objectID":"96a177806f5aa6d570b2323e004110ac","permalink":"https://sbjoshi.github.io/project/dora/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/project/dora/","section":"project","summary":"Scaleable, transparent and efficient Distributed Oracle Agreement","tags":["Blockchain","Distributed Systems"],"title":"DORA","type":"project"},{"authors":["Utpal Bora","Shraiysh Vaishay","Saurabh Joshi","Ramakrishna Upadrasta"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636439023,"objectID":"c4b7a0f0ac8c6b207a6754d1180f9ece","permalink":"https://sbjoshi.github.io/publication/llov-mhp-llvmhpc21/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/publication/llov-mhp-llvmhpc21/","section":"publication","summary":"This paper employs MHP analysis inside LLOV, to increase coverage of OpenMP pragmas for data-race checking for OpenMP Programs.","tags":["Program Analysis"],"title":"OpenMP aware MHP Analysis for Improved Static Data-Race Detection","type":"publication"},{"authors":["Archit Sanghi","Praveen Tammana","Saurabh Joshi","Krishna P Kadiyala"],"categories":null,"content":"","date":1629849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635744851,"objectID":"3f2488f465c8cfb2d705034276295870","permalink":"https://sbjoshi.github.io/publication/spin2021-anomaly-detection/","publishdate":"2021-06-25T00:00:00Z","relpermalink":"/publication/spin2021-anomaly-detection/","section":"publication","summary":"This paper describes a statistical analysis based technique to detect certain kinds of attack on the programmable data planes.","tags":["Cybersecurity","Statistical Analysis"],"title":"Anomaly Detection in Data Plane Systems using Packet Execution Paths","type":"publication"},{"authors":["Saurabh Joshi","Gautam Muduganti"],"categories":null,"content":"","date":1610409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611041445,"objectID":"dc2e8f39e9f55297c3c27b52b379ea34","permalink":"https://sbjoshi.github.io/publication/gpurepair-vmcai21/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/publication/gpurepair-vmcai21/","section":"publication","summary":"This paper presents a tool and a technique to fix data-race and barrier divergence errors in CUDA and OpenCL programs.","tags":["Formal Verification"],"title":"GPURepair: Automated Repair of GPU Kernels","type":"publication"},{"authors":["Utpal Bora","Santanu Das","Pankaj Kukreja","Saurabh Joshi","Ramakrishna Upadrasta","Sanjay V Rajopadhye"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612502443,"objectID":"a5eb136eb2a737801309f2bcaa640c8f","permalink":"https://sbjoshi.github.io/publication/llov-taco20/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/publication/llov-taco20/","section":"publication","summary":"This paper presents a tool, LLOV, which leverages polyhedral compilation for fast data-race checking for OpenMP Programs.","tags":["Program Analysis"],"title":"LLOV: A Fast Static Data-Race Checker for OpenMP Programs","type":"publication"},{"authors":["Sriram Bhyravarapu","Saurabh Joshi","Subrahmanyam Kalyanasundaram","Anjeneya Swami Kare"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602766831,"objectID":"b637f3ac96c5cfa9045e75d7afc90288","permalink":"https://sbjoshi.github.io/publication/kicoloring/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/publication/kicoloring/","section":"publication","summary":"This paper is primarily about NP-completeness of $(k,i)$-coloring of graph and giving a parameterized algorithm for this problem using feedback vertex set as the parameter.","tags":["CS Theory"],"title":"On the tractability of $(k,i)$-coloring (Extended Journal Version)","type":"publication"},{"authors":["Ruben Martins","Saurabh Joshi","Vasco Manquinho","Ines Lynce"],"categories":null,"content":"","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602846970,"objectID":"e80504c14e255fa6b19c68668d69c382","permalink":"https://sbjoshi.github.io/publication/reflections-virtualvolume-cp19/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/publication/reflections-virtualvolume-cp19/","section":"publication","summary":"To celebrate the first 25 years of the International Conference on Principles and Practice of Constraint Programming (CP) the editors invited the authors of the most cited paper of each year to write a commentary on their paper. This report describes our reflections on the CP 2014 paper \"Incremental Cardinality Constraints for MaxSAT\" and its impact on the Maximum Satisfiability community and beyond.","tags":["Constraint Programming"],"title":"Reflections on \"Incremental Cardinality Constraints for MaxSAT\"","type":"publication"},{"authors":["Saurabh Joshi","Prateek Kumar","Sukrut Rao","Ruben Martins"],"categories":null,"content":"","date":1567296e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608188544,"objectID":"dd6170840c6d44399fe22285c7d8c920","permalink":"https://sbjoshi.github.io/publication/open-wbo-inc-jsat/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/publication/open-wbo-inc-jsat/","section":"publication","summary":"This paper presents a couple of incomplete Weighted MaxSAT solving techniques along with analysis on the deviation to the optimal value.","tags":["Constraint Programming"],"title":"Open-WBO-Inc: Approximation Strategies for Incomplete Weighted MaxSAT","type":"publication"},{"authors":["Yash Pote","Saurabh Joshi","Kuldeep Meel"],"categories":null,"content":"","date":1567296e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602598933,"objectID":"abbd28be6439d61e161fd2ce2e2ebd76","permalink":"https://sbjoshi.github.io/publication/phasetransition-ijcai19/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/publication/phasetransition-ijcai19/","section":"publication","summary":"This paper studies phase transition behaviour of CARD-XOR formulas.","tags":["Constraint Programming"],"title":"Phase Transition Behavior of Cardinality and XOR Constraints","type":"publication"},{"authors":["Eti Chaudhary","Saurabh Joshi"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608189386,"objectID":"f5e270bffa4c0c701f270e78c3e6dca5","permalink":"https://sbjoshi.github.io/publication/pinaka-tacas19/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/publication/pinaka-tacas19/","section":"publication","summary":"This paper describes, Pinaka, a symbolic execution engine that leverages incremental SAT solving.","tags":["Formal Verification"],"title":"Pinaka: Symbolic Execution Meets Incremental Solving - (Competition Contribution)","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608188084,"objectID":"c3decd8ed8d3b9761c9b404b98874540","permalink":"https://sbjoshi.github.io/project/llov/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project/llov/","section":"project","summary":"A static data-race checker for OpenMP programs.","tags":["Program Analysis"],"title":"LLOV","type":"project"},{"authors":["Saurabh Joshi","Prateek Kumar","Ruben Martins","Sukrut Rao"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608188544,"objectID":"19d838cff7fe52274041c7cd4ba305e7","permalink":"https://sbjoshi.github.io/publication/open-wbo-inc-cp18/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/publication/open-wbo-inc-cp18/","section":"publication","summary":"This paper presents a couple of incomplete Weighted MaxSAT solving techniques that allowed _Open-WBO-Inc_ to win accolades in MaxSAT evaluations 2018 and MaxSAT evaluations 2019.","tags":["Constraint Programming"],"title":"Approximation Strategies for Incomplete MaxSAT","type":"publication"},{"authors":null,"categories":null,"content":"Open-WBO-Inc Open-WBO-Inc is an open-source incomplete solver for MaxSAT. Often, real-world applications demand a good answer to MaxSAT optimization problem very quickly. Incomplete solvers strive to find a good answer very fast but may not guarantee optimality of the provided answer.\nOpen-WBO-Inc has won several accolades at international arena. In addition, it serves as a platform on top of which other award winning solvers such as Loandra and TT-Open-WBO-Inc has been built.\nMaxSAT Evaluation 2019: 2 Bronze MaxSAT Evaluation 2018: 1 Gold, 1 Silver ","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608187849,"objectID":"bfbf4b0122299a81eadbe909e628f0e8","permalink":"https://sbjoshi.github.io/project/openwboinc/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/project/openwboinc/","section":"project","summary":"An open-source solver for incomplete MaxSAT","tags":["Constraint Programming"],"title":"Open-WBO-Inc","type":"project"},{"authors":["Saurabh Joshi","Subrahmanyam Kalyanasundaram","Anjeneya Swami Kare","B Sriram"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602766831,"objectID":"e899c492ef188edc77e305fb3c8ec315","permalink":"https://sbjoshi.github.io/publication/kicoloring-caldam18/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/publication/kicoloring-caldam18/","section":"publication","summary":"This paper is primarily provides a parameterized algorithm for $(k,i)$-coloring  problem using feedback vertex set as the parameter.","tags":["CS Theory"],"title":"On the tractability of $(k,i)$-coloring","type":"publication"},{"authors":null,"categories":null,"content":"GPURepair GPURepair is a tool that can propose a fix to data-race and barrier-divergence errors in CUDA and OpenCL kernels. It uses GPUVerify as an oracle. It can also propose a fix for inter-block data-race in CUDA kernels using CUDA Cooperative Groups.\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608188156,"objectID":"ac2f2e8f7bdb1cbbe5c87d82b058fdc2","permalink":"https://sbjoshi.github.io/project/gpurepair/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/project/gpurepair/","section":"project","summary":"A tool for automated repair of CUDA and OpenCL kernels","tags":["Formal Verification","Automated Program Repair"],"title":"GPURepair","type":"project"},{"authors":null,"categories":null,"content":"OpenWBO OpenWBO is a modular open-source sover for MaxSAT and Pseudo-Boolean formulas. OpenWBO has won several accolades in various international arena.\nMaxSAT Evaluation 2017: 2 Gold, 1 Silver MaxSAT Evaluation 2016: 1 Gold, 1 Silver Pseudo-Boolean Evaluation 2016: 2 Silver, 2 Bronze MaxSAT Evaluation 2015 : 1 Gold, 1 Silver MaxSAT FLoC Olympic Games 2014 : 2 Gold MaxSAT Evaluation 2014 : 1 Gold, 1 Silver ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608187221,"objectID":"b3c7ef6ee43ad1c9dfb37a5144110647","permalink":"https://sbjoshi.github.io/project/openwbo/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/project/openwbo/","section":"project","summary":"An open-source MaxSAT and Pseudo-Boolean solver","tags":["Constraint Programming"],"title":"OpenWBO","type":"project"},{"authors":null,"categories":null,"content":"Pinaka: Symbolic Execution meets Incremental Solving Symbolic execution as well as incremental solving has been quite well known concepts to the researchers and students working in the field of Formal Methods.\nToday I am going to write about Pinaka, a symbolic execution engine that combines symbolic execution with incremental solving in a fairly simple fashion.\nIf you are merely looking at documentation about how to use it, please refer to the post on \u0026ldquo;How to use Pinaka\u0026rdquo;.\nIncremental Solving Many modern day constraint solvers, such as SAT and SMT solvers support incremental solving in some fashion. Say, for some problem, we require solutions for the following set of formulas: $\\varphi, \\varphi \\wedge \\Delta_1, \\varphi \\wedge \\Delta_1 \\wedge \\Delta_2$\nOne of the ways to solve these is to create a solver instance for each of these formulas separately and ask the solver for a solution for each of these separately. Note however, that any assignment/model/interpretation that does not satisfy $\\varphi$ is not going to satisfy the other two. Similarly, any assignment that does not satisfy $\\varphi\\wedge\\Delta_1$ is not going to satisfy the last formula. Essentially, adding more constraints to the existing formula is going to preserve the fact that assignments that falsifies a formula continue to do so even after more constraints are added.\nIt would result in a huge performance gain, if we use the same solver instance to solve all the three formulas. When we reuse the solver instance, the internal state of the solver including variable and clause activity scores, learned clauses, etc., are carried over and reused for the next query.\nThis way of solving a set of formulas with successive queries to the same solver will be denoted as partial incremental mode.\nNow, suppose we have the following set of formulas: $\\varphi \\wedge \\Delta_1, \\varphi \\wedge \\Delta_2, \\varphi \\wedge \\Delta_3$. Note that, though $\\varphi$ remains common, it is no longer the case that one formula is just addition of constraints over the other. The second formula can be obtained from the first by deleting $\\Delta_1$ and adding $\\Delta_2$. However, the moment you delete a set of constraints, the assignment that used to falsify the old formula may no longer falsify the new formula. Therefore, the search space that was discarded earlier by the solver has to be explored again. The internal state of the solver can not easily be reused in such a scenario.\nHowever, many modern day solvers have another trick up their sleeve, called assumptions. Assumptions are a list of literals that can be logically thought of as added unit clauses. When you know that some constraint may no longer be required for successive queries, you can add that constraint being enforced by an assumption. Therefore, the first formula, can be rewritten as $\\varphi \\wedge (b_1 \\implies \\Delta_1) \\wedge [b_1]$. Here, $[b_1]$ is an assumption, which can logically be thought as a unit clause. Therefore, making this formula equisatisfiable to $\\varphi \\wedge \\Delta_1$. To transform, the first formula to the second one, now we can add $(b_2\\implies \\Delta_2)$ and change the set of assumptions to $[\\neg b_1,b_2]$. So the second formula now looks like:\n$$\\varphi \\wedge (b_1\\implies \\Delta_1) \\wedge (b2\\implies \\Delta_2) \\wedge [\\neg b_1,b_2]$$.\nWhen you think that assumptions are logically just unit clauses, then the above formula in some sense deletes $\\Delta_1$ because the premise of the implicant is false. Therefore, we are transforming one formula to the next just by adding $(b_i \\implies \\Delta_i)$ and changing assumptions. Note that we are changing $[b_1]$ to $[\\neg b_1]$, which looks like we are deleting $b_1$ and adding $\\neg b_1$, but solvers treat assumptions in a spacial manner internally. It is still the case that the assignment that falsifies the first formula may no longer falsify the second formula. However, using assumptions still allow a lot of information pertaining to $\\varphi$ to be carried over to the next query. Thus, assumptions allow us to salvage at least some information, which would be lost if we were to create a different solver instance for each query.\nOf course, adding such $(b_i \\implies \\Delta_i)$ for a lot of successive queries result in degradation of performance because of large memory footprint and slower iteration over internal data-structures. However, assumptions allows us to reuse a solver instance and can give great performance benefits if the common subformula $\\varphi$ is very large and $\\Delta_i$ are small and few in numbers.\nThis way of using the same solver instance for successive queries will be denoted as full incremental mode.\nSymbolic Execution Let\u0026rsquo;s take a look at a motivating example for symbolic execution.\nLook at the subroutine foo given below. Note that the assertion violation will happen only when variable y gets -10 as its value at line 1. Assuming an int to be of 32 bits, and if you were to test this subroutine by generating values of y uniformly randomly and running foo, then the probability of you finding the bug will be $2^{-32}$.\n1 void foo(int y) { 2 int x=10; 3 x=x+y; 4 if(x==0) { 5 x=x+1; 6 if(y\u0026lt;-10) 7 x=x-1; 8 } 9 else { 10 x=0; 11 } 12 assert(x==0); 13 } The problem with testing, as mentioned above, is that a single run only exposes program behaviour for a single input value along a single path.\nIn symbolic execution, it is possible to explore program behaviour for a set of input values along a single path. We will not talk about multi-path symbolic execution in this post.\nSymbolic execution uses a predicate at every program point along a path to represent the set of values program variables can take along the given path. This predicate is also often called a symbolic state.\nFor example, the symbolic state after executing lines 1-3 would be :\n$$y_0 \\wedge x_0 = 10 \\wedge x_1 = x_0 + y_0 \\quad (1) $$\nHere, $x_0,x_1,y_0$ are symbolic /mathematical variables that represent the set of values that the corresponding program variable can take at corresponding program points. For example, $(x_0,10), (x_1,11),(y_0,3)$ as a program state is not possible after executing line 3 as it does not satisfy the predicate mentioned above. On the other hand $(x_0,10),(x_1,12),(y_0,2)$ satisfies the predicate, indicating that after executing lines 1-3 program variable x can be 12 and y can be 2. Similarly, $(x_0,10),(x_1,0),(y_0,-10)$ is also a solution to the predicate indicating that x taking the value 0 and y taking the value -10 at line 4 is a possibility.\nLine 4 is a branch and gives rise to two symbolic states. Symbolic state for the then branch is: $$y_0 \\wedge x_0 = 10 \\wedge x_1 = x_0 + y_0\\wedge x_1=0\\quad(2)$$\nThe symbolic state for the else branch is: $$y_0 \\wedge x_0 = 10 \\wedge x_1 = x_0 + y_0\\wedge x_1\\neq0\\quad(3)$$.\nPinaka being a single path execution engine, maintains a worklist of symbolic states that are yet to be explored further. Therefore, at a branch, it picks one state to be explored further and puts the other one on the worklist. Say, it picked the symbolic state for the then branch. Pinaka also employs eager feasibility checks. In other words, at every branch, it poses a query to a constraint solver whether the symbolic state is feasible or not. If there is no solution to the predicate, then the corresponding symbolic state is deemed to be infeasible, indicating that in an actual/concrete execution, it is not possible for the control to reach this point.\nIn our example, then branch at line 4 is feasible. Symbolic state for the then branch at line 6would be:\n$$y_0 \\wedge x_0 = 10 \\wedge x_1 = x_0 + y_0\\wedge x_1=0 \\ \\wedge x_2=x_1+1 \\wedge y_0\u0026lt;-10 \\quad(4) $$\nSymblic state for (implicit) else at line 8 would be: $$y_0 \\wedge x_0 = 10 \\wedge x_1 = x_0 + y_0\\wedge x_1=0 \\ \\wedge x_2=x_1+1 \\wedge y_0\\geq -10 \\quad(5) $$\nPinaka uses incremental solving aggressively. Note that the symbolic state for then branch at line 4 is of the form $\\varphi$ and the symbolic state for the then branch at line 6 differs by two additional constraint $x_2=x_1+1 \\wedge y_0\u0026lt;-10$ which can be considered as $\\Delta_1$ as mentioned in earlier section. Therefore, along a single path, Pinaka continues to reuse the same solver instance as long as the symbolic state remains feasible.\nObserve that this symbolic state given in $(4)$ above is infeasible and therefore, in a real execution line 7 is unreachable irrespective of the input value of y.\nIn the DFS (depth first search) exploration strategy, Pinaka picks the last unexplored symbolic state from the list. This can easily be achieved when we operate the worklist of symbolic states pending further exploration in a LIFO (Last In First Out) manner. In this example, it would pick the symbolic state mentioned at (5) above, corresponding to the if condition being false at line 6.\nIn the partial-incremental mode, once a symbolic state is found to be infeasible, a new solver instance is created to explore the yet unexplored symbolic state, in this case, (5) above.\nIn the full incremental mode, the predicate/formula at line 6 would have looked like the following: $$y_0 \\wedge x_0 = 10 \\wedge x_1 = x_0 + y_0 \\ \\wedge (b_1 \\implies x_1=0) \\wedge (b_1 \\implies x_2=x_1+1) \\ \\wedge (b_1 \\implies (b_2 \\implies y_0\u0026lt;-10)) \\wedge [b_1,b_2] \\quad(6) $$\nNotice that a new assumption is added for every branch condition as later we may want to backtrack from a then branch and explore an else branch. So in the full incremental mode, the same solver instance will be reused after the symbolic state at line 6 for the then branch is found to be infeasible. For the else branch, the formula would look like the following: $$y_0 \\wedge x_0 = 10 \\wedge x_1 = x_0 + y_0 \\ \\wedge (b_1 \\implies x_1=0) \\wedge (b_1 \\implies x_2=x_1+1) \\ \\wedge (b_1 \\implies (b_2 \\implies y_0\u0026lt;-10)) \\ \\wedge (b_1 \\implies (\\neg b_2 \\implies y\\geq -10)) \\wedge [b_1,\\neg b_2] \\quad(7) $$\nOnce the symbolic execution has gone through the path depicted by lines 1-6, 8,11 we have to check for assertion violation. Essentially, we again want to know, if there is any set of values that program variables can take that can result in assertion violation. This can be done by asking if the following symbolic state is feasible:\n$$y_0 \\wedge x_0 = 10 \\wedge x_1 = x_0 + y_0\\wedge x_1=0 \\ \\wedge x_2=x_1+1 \\wedge y_0\\geq -10 \\wedge x_2\\neq 0 \\quad(8) $$\nObserve that we negated the condition inside the assertion as we want to know if the program variables can take a set of values that violates (negates) the condition given in the assertion. Note that (8) above is satisfiable with $(y_0,-10)$, thus revealing that foo can fail if the value of the parameter y is -10.\nNumber of paths in a program can grow exponentially in proportion to the number of branches in the program. This path explosion problem can pose quite a problem for single-path symbolic execution engines such as Pinaka. Eager infeasibility checks employed by Pinaka gives it tremendous performance boost as all the paths beyond an infeasible symbolic state is discarded. However, doing eager infeasibility checks would mean that every time a branch is encountered, a solver query has to be fired. Since querying a solver is expensive, increasing the number of queries can pose another challenge. That is when integration of incremental solving can provide tremendous benefit.\nTermination Loops and recursions are primary programming constructs in a programming language that causes non-termination in a program.\nPinaka performs loop-unrolling and function in-lining lazily and on-demand. This feature allows it to be used to check for termination in some cases.\nLet\u0026rsquo;s take a look at the following program fragment:\n1 while(x \u0026lt; 10) { 2 y = y + 1; 3 if(y \u0026lt; 5) { 4 x = x + 1 5 } 6 } Note that for all the values of x and y, if they satisfy $x-y\u0026gt;5 \\vee x\\geq 10$, this loop will terminate. It may not terminate otherwise. However, Pinaka does not compute this relation. Instead, it treats the loop condition just as a branch. So the loop will keep being unrolled on-demand as long as the symbolic state at line 1 continues to remain feasible. In another words, if there is any set of values for x and y which would make it possible for the loop to iterate at least one more time, Pinaka will also go around the loop symbolically executing the loop at least one more time. As a consequence, for a program which does not violate any assertions (safe programs), Pinaka will terminate only if the program itself is terminating. Of course, we are assuming that the solver queries themselves terminate and Pinaka does not run out of memory and time.\nFor programs, that may violate assertions, this guarantee does not hold. Since if Pinaka finds an assertion violation along some path, it will terminate even if there exist paths in the programs which are non-terminating.\nDo it yourself Those who want to try this example out and compare testing against symbolic execution, following is the C++ program for testing:\n#include\u0026lt;random\u0026gt; #include\u0026lt;limits\u0026gt; #include\u0026lt;iostream\u0026gt; #include\u0026lt;cassert\u0026gt; void foo(int y) { int x=10; x=x+y; if(x==0) { x=x+1; if(y\u0026lt;-10) x=x-1; } else { x=0; } assert(x==0); } int main() { std::random_device rseed; std::mt19937 rng(rseed()); std::uniform_int_distribution\u0026lt;int\u0026gt; dist(std::numeric_limits\u0026lt;int\u0026gt;::min(),std::numeric_limits\u0026lt;int\u0026gt;::max()); unsigned i=0; unsigned max=1000000; //unsigned max=std::numeric_limits\u0026lt;unsigned\u0026gt;::max(); while(i\u0026lt;max) { std::cout\u0026lt;\u0026lt; \u0026quot;i : \u0026quot; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; foo(dist(rng)); i++; }; return 0; } Compile, run and observe using the following commands:\n$ g++ -o footest filename.cpp $ ./footest 2\u0026gt;\u0026amp;1 \u0026gt; foorun.log $ tail foorun.log I could not trigger the assertion violation when I tried. In contrast when you run Pinaka using the following command you will get assertion violation notification in a fraction of a second.\n$ pinaka --function foo filename.c References Pinaka binaries are available here. Pinaka performed reasonably well and quite fast in SVCOMP 2019 and SVCOMP 2020 You may also refer to the following paper: Eti Chaudhary and Saurabh Joshi, \u0026ldquo;Pinaka: Symbolic Execution meets Incremental Solving\u0026rdquo;, TOOLympics, TACAS (3), LNCS Vol. 11529, pp. 234\u0026ndash;238, 2019. Pre-print version is available on arXiv. ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636991059,"objectID":"efc1767583faf2bd61a84a05cc10030e","permalink":"https://sbjoshi.github.io/project/pinaka/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/project/pinaka/","section":"project","summary":"A symbolic execution engine","tags":["Formal Verification"],"title":"Pinaka","type":"project"},{"authors":["Vojtech Forejt","Saurabh Joshi","Daniel Kroening","Ganesh Narayanaswamy","Subodh Sharma"],"categories":null,"content":"","date":1504224e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602847641,"objectID":"d76365442507dc9925337c09a1b4c994","permalink":"https://sbjoshi.github.io/publication/predictiveanalysis-toplas/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/publication/predictiveanalysis-toplas/","section":"publication","summary":"This paper shows NP-completeness of deadlock detection in certain class of MPI programs. It also presents encoding to analyze a class of MPI programs with respect to deadlocks.","tags":["Formal Verification","Program Analysis"],"title":"Precise Predictive Analysis for Discovering Communication Deadlocks in MPI Programs","type":"publication"},{"authors":["Rajdeep Mukherjee","Saurabh Joshi","Andreas Griesmayer","Daniel Kroening","Tom Melham"],"categories":null,"content":"","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602848280,"objectID":"c3ff364d50d810c081446ec3bafce5a6","permalink":"https://sbjoshi.github.io/publication/equivalencefpu-fm16/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/publication/equivalencefpu-fm16/","section":"publication","summary":"This paper describes an efficient technique for equivalence checking of a real-world Floating Point Unit.","tags":["Formal Verification"],"title":"Equivalence Checking of a Floating-Point Unit Against a High-Level C Model","type":"publication"},{"authors":["Ganesh Narayanaswamy","Saurabh Joshi","Daniel Kroening"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602851324,"objectID":"b6a3da14e4e00062ccc1ff64e07cd278","permalink":"https://sbjoshi.github.io/publication/concurrency-ppoppp16/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/publication/concurrency-ppoppp16/","section":"publication","summary":"This paper presents a different encoding that makes Bounded Model Checking faster for concurrent programs.","tags":["Formal Verification","Program Analysis"],"title":"The virtues of conflict: analysing modern concurrency","type":"publication"},{"authors":["Ruben Martins","Saurabh Joshi","Vasco Manquinho","Ines Lynce"],"categories":null,"content":"","date":1446336e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608189386,"objectID":"4c86d5a296855bfbb678e53b9700a8c2","permalink":"https://sbjoshi.github.io/publication/inc-cardinality-jsat14/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/publication/inc-cardinality-jsat14/","section":"publication","summary":"This paper is an extended version of the CP 2014 paper where incremental encoding is extended to weighted MaxSAT.","tags":["Constraint Programming"],"title":"On Using Incremental Encodings in Unsatisfiability-based MaxSAT Solving","type":"publication"},{"authors":["Saurabh Joshi","Ruben Martins","Vasco Manquinho"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608188544,"objectID":"83447e97b4b7488544f55a8909f9d1fa","permalink":"https://sbjoshi.github.io/publication/gte-cp15/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/publication/gte-cp15/","section":"publication","summary":"This paper describes Generalized Totalizer Encoding (GTE) to encode Pseudo-Boolean Constraints. This encoding led Open-WBO to win accolades in MaxSAT evaluations and Pseudo-Boolean evaluations.","tags":["Constraint Programming"],"title":"Generalized Totalizer Encoding for Pseudo-Boolean Constraints","type":"publication"},{"authors":["Martin Brain","Saurabh Joshi","Daniel Kroening","Peter Schrammel"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602854149,"objectID":"660c846cef8fef1e77f003e565f07790","permalink":"https://sbjoshi.github.io/publication/kiki-sas15/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/publication/kiki-sas15/","section":"publication","summary":"This paper describes a sound and complete tool, 2LS,  for program verification and the techniques behind its working.","tags":["Formal Verification","Program Analysis"],"title":"Safety Verification and Refutation by $k$-Invariants and $k$-Induction","type":"publication"},{"authors":["Saurabh Joshi","Daniel Kroening"],"categories":null,"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608191145,"objectID":"16689fd18e06244add901b20110c2120","permalink":"https://sbjoshi.github.io/publication/robmc-fm15/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/publication/robmc-fm15/","section":"publication","summary":"This paper introduces Re-Order Bounded Model Checking to efficiently repair programs on weak memory models.","tags":["Formal Verification","Program Analysis","Automated Program Repair"],"title":"Property-Driven Fence Insertion Using Reorder Bounded Model Checking","type":"publication"},{"authors":["Ruben Martins","Saurabh Joshi","Vasco Manquinho","Ines Lynce"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608188544,"objectID":"d17d6bb17264a791308b93632bb7a690","permalink":"https://sbjoshi.github.io/publication/inc-cardinality-cp14/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/publication/inc-cardinality-cp14/","section":"publication","summary":"This paper describes techniques to incremental encode cardinality constraints. This led Open-WBO to win accolades in MaxSAT evaluations.","tags":["Constraint Programming"],"title":"Incremental Cardinality Constraints for MaxSAT","type":"publication"},{"authors":["Saurabh Joshi","Akash Lal"],"categories":null,"content":"","date":1393632e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602859661,"objectID":"9fa8ebe152671397de3e5b1e3ac16de9","permalink":"https://sbjoshi.github.io/publication/atomicinf-arxiv/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/publication/atomicinf-arxiv/","section":"publication","summary":"This paper presents a technique for automatically constructing a fix for buggy concurrent programs: given a concurrent program that does not satisfy user-provided assertions, we infer atomic blocks that fix the program. An atomic block protects a piece of code and ensures that it runs without interruption from other threads. Our technique uses a verification tool as a subroutine to find the smallest atomic regions that remove all bugs in a given program. Keeping the atomic regions small allows for maximum concurrency. We have implemented our approach in a tool called AtomicInf. A user of AtomicInf can choose between strong and weak atomicity semantics for the inferred fix. While the former is simpler to find, the latter provides more information about the bugs that got fixed.  We ran AtomicInf on several benchmarks and came up with the smallest and the most precise atomic regions in all of them. We implemented an earlier technique to our setting and observed that AtomicInf is 1.7 times faster on an average as compared to an earlier approach.","tags":["Formal Verification","Automated Program Repair"],"title":"Automatically finding atomic regions for fixing bugs in Concurrent Programs","type":"publication"},{"authors":["Saurabh Joshi","R K Shyamasundar","Sanjeev K Aggarwal"],"categories":null,"content":"","date":133056e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602856280,"objectID":"fdfa96c459424c72b406847e192ddb48","permalink":"https://sbjoshi.github.io/publication/mhp-ipdpsw12/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/publication/mhp-ipdpsw12/","section":"publication","summary":"This paper presents a technique to perform May-Happen-in-Parallel analysis for languages with Dynamic Barriers.","tags":["Program Analysis"],"title":"A New Method of {MHP} Analysis for Languages with Dynamic Barriers","type":"publication"},{"authors":["Saurabh Joshi","Shuvendu Lahiri","Akash Lal"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602855511,"objectID":"716fae8dbf22f940a20d1ca6929e30dc","permalink":"https://sbjoshi.github.io/publication/underspecified-popl12/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/publication/underspecified-popl12/","section":"publication","summary":"This paper presents a technique to find interleaved bugs even with incomplete harness.","tags":["Formal Verification","Program Analysis"],"title":"Underspecified harnesses and interleaved bugs","type":"publication"},{"authors":["Shivali Agarwal","Saurabh Joshi","R K Shyamasundar"],"categories":null,"content":"","date":129384e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602858718,"objectID":"2633ec364a17f683d2f8f8015c416de7","permalink":"https://sbjoshi.github.io/publication/icdcn11/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/publication/icdcn11/","section":"publication","summary":"Static assertion checking of open programs requires setting up a precise harness to capture the environment assumptions. For instance, a library may require a file handle to be properly initialized before it is passed into it. A harness is used to set up or specify the appropriate preconditions before invoking methods from the program. In the absence of a precise harness, even the most precise automated static checkers are bound to report numerous false alarms. This often limits the adoption of static assertion checking in the hands of a user.  In this work, we explore the possibility of automatically filtering away (or prioritizing) warnings that result from imprecision in the harness. We limit our attention to the scenario when one is interested in finding bugs due to concurrency. We define a warning to be an interleaved bug when it manifests on an input for which no sequential interleaving produces a warning. As we argue in the paper, limiting a static analysis to only consider interleaved bugs greatly reduces false positives during static concurrency analysis in the presence of an imprecise harness.  We formalize interleaved bugs as a differential analysis between the original program and its sequential version and provide various techniques for finding them. Our implementation CBugs demonstrates that the scheme of finding interleaved bugs can alleviate the need to construct precise harnesses while checking real-life concurrent programs.","tags":["Program Analysis"],"title":"Distributed Generalized Dynamic Barrier Synchronization","type":"publication"},{"authors":["Frederic Doucet","R K Shyamasundar","Ingolf Krueger","Saurabh Joshi","Rajesh K Gupta"],"categories":null,"content":"","date":1191196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602859210,"objectID":"2da4877f4cae746c7fce316515f92755","permalink":"https://sbjoshi.github.io/publication/hvc07/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/publication/hvc07/","section":"publication","summary":"SystemC is a popular language used in modeling system-on-chip implementations. To support this task at a high level of abstraction, transaction-level modeling (TLM) libraries have been recently developped. While TLM libraries are useful, it is difficult to capture the reactive nature of certain transactions with the constructs currently available in the SystemC and TLM libraries. In this paper, we propose an approach to specify and verify reactive transactions in SystemC designs. Reactive transactions are different from TLM transactions in the sense that a transaction can be killed or reset. Our approach consists of: (1) a language to describe reactive transactions that can be translated to verification monitors, (2) an architectural pattern to implement reactive transactions, and (3) the verification support to verify that the design does not deadlock, allows only legal behaviors and is always responsive. We illustrate our approach through an example of a transactional memory system where a transaction can be killed or reset before its completion. We identify the architectural patterns for reactive transactions. Our results demonstrate the feasibility of our approach as well as support for a comprehensive verification using RuleBase/NuSMV tools.","tags":["Formal Verification"],"title":"Reactivity in SystemC Transaction-Level Models","type":"publication"},{"authors":null,"categories":null,"content":"How to use Pinaka Pinaka is a single-path symbolic execution engine, built on top of CPROVER framework.\nIn the following text, we will show how to use Pinaka through various examples.\nUsing assert to check properties Pinaka verifies a program with respect to given specifications. These specifications are typically given as a set of program assertions using assert statements in the program.\n1 #include \u0026lt;assert.h\u0026gt; 2 int main() 3 { 4 int a, b=5; 5\tint c = a+b; 6\tassert(c\u0026lt;100); 7\treturn 0; 8 } Look at the example shown above. The C language standard does not forbid the use of uninitialized variable, however, using uninitialized variables may lead to undefined behavior. Here, a is uninitialized and therefore, as per the C language standard, it is free to to take any value within the domain of int.\nAs, you can observe, on line 6, assertion has been given. Programmers or user of Pinaka is supposed to use assert to indicate what properties they want Pinaka to verify the program against.\nOn the above program, the following command shall be issued:\npinaka filename.c\nPinaka will show the following output:\nRunning with 8 object bits, 56 offset bits (default) Parsing /tmp/try4.c \u0026lt;command-line\u0026gt;: warning: \u0026quot;__STDC_VERSION__\u0026quot; redefined \u0026lt;built-in\u0026gt;: note: this is the location of the previous definition Converting Type-checking try4 Generating GOTO Program Adding CPROVER library (x86_64) Generic Property Instrumentation Removal of function pointers and virtual functions Depth First Search Full Incremental Mode OUR FUNCTION CALLED Number of dropped states: 0 Generated 1 VCC(s), 1 remaining after simplification Number of SAT queries made: 0 Number of new SAT instances: 0 Number of total paths: 0 Number of feasible path: 1 Number of infeasible path: 0 Runtime: 0.036s total, 0.001s SAT [main.assertion.1] assertion c\u0026lt;100: FAILED ** 1 of 1 failed VERIFICATION FAILED (ReachSafety) Note that without any assertions, Pinaka does not have any specification to verify the program. Therefore the following program would result in VERIFICATION SUCCESSFULL.\n1 #include \u0026lt;assert.h\u0026gt; 2 int main() 3 { 4 int a, b=5; 5\tint c = a+b; 6\treturn 0; 7 } printf statements are irrelevant for Pinaka as it does not consider any printf as part of the specification.\nVerifying functions other than main and signed integer overflow check By default, Pinaka assumes that the entry point, or starting point for analysis would be main subroutine. However, one can specify a subroutine other than main as well and verify the subroutine.\nFor example,\nint inc(int x) { return ++x; } Let us assume that we want to check for signed integer overflow for inc subroutine. We should use the following command\npinaka --signed-overflow-check --function inc filename.c\nNote, that we combined two command line options here.\n--signed-overflow-check tells Pinaka to use signed integer arithmetic overflow as the property to be verified. In such a case, we do not have to provide any assert statements in the program. --function inc tells Pinaka to use subroutine inc as the starting point for the analysis. In this case, the source file need not have a main subroutine inside it. Even if it is present, it will be ignored . Array index out-of-bounds check and showing the trace Pinaka supports array index out-of-bounds check as in-built property.\nFor example,\n#include \u0026lt;assert.h\u0026gt; int main() { int a[] = {0,1,2,3,4}; int i,sum=0; for (i=0;i\u0026lt;=5;i++) { sum+=a[i]; assert(sum\u0026gt;=0); } return 0; } Let us first try without the array index out-of-bounds check.\npinaka filename.c\nThe result would be VERIFICATION FAILED because the sum can indeed become negative since a[5] can be -11 or less. Note that the above program would compile successfully.\nIf we also want to see the trace/potential execution through which the program would fail we should issue the following command:\npinaka --show-trace filename.c\nThe trace would clearly show that the value for a[5] would be so much negative so that sum becomes negative. We can observe that this issue arises as the program tries to access an array index which is outside the legal range of the array.\nTo perform the array index out-of-bounds check, the following command shall be issued:\npinaka --bounds-check --show-trace filename.c\nNow it will show that the upper bound on array index a is violated.\nPinaka search modes Pinaka supports various internal modes, each with having their own advantages and disadvantages in terms of speed and memory consumption.\nFor the search, two modes are supported, Since, Pinaka is a single path symbolic execution engine, the default mode of the search is depth-first search. If you wish to use breadth-first search instead, please use the following command.\npinaka --bfs filename.c\nSimilarly, the default incremental mode of Pinaka is full incremental mode. If, you wish to use partial incremental mode, please use the following command.\npinaka --partial-incremental filename.c\nNote that we DO NOT recommend to use partial incremental mode with breadth-first search as it can quickly consume a lot of memory.\nFor SVCOMP, we found partial incremental mode with the deapth-first search to be the best combination to use. So the command to use this combination would be\npinaka --partial-incremental filename.c\nBit-width options Pinaka has options to specify the width for a word. For example, you can choose if the width of a word is 16-bit, 32-bit or 64 bit. Pinaka internally uses a technique called bit-blasting to provide a bit-precise analysis of the program. For example, the specified bit-width would determine when to flag for an overflow, underflow for integers.\nThese options can be used as follows:\npinaka --64 filename.c\nSimilarly, to specify bit-width of 16 or 32 bits, one can use --16 or --32 respectively.\nFloating-point rounding modes Pinaka supports four rounding modes for floating-point arithmetic. Floating-point modelling is compliant to IEEE 754 standard.\nThere are four rounding modes:\nRound to nearest (--round-to-nearest) Round to $+\\infty$ (--round-to-plus-inf) Round to $-\\infty$ (--round-to-minus-inf) Round to $0$ (--round-to-zero) One can refer to this Wikipedia article to know more about rounding modes.\nDivide by zero check You can use --div-by-zero-check to tell Pinaka to detect potential division by zero.\n#include \u0026lt;assert.h\u0026gt; int main() { int a,b; int c = a/b; assert(c\u0026gt;0); return 0; } Look at the example above. There is a potential division by $0$ since b being an uninitialized variable, can potentially have the value of 0.\nYou can use the following command to tell Pinaka to check for such errors as follows:\npinaka --div-by-zero-check filename.c\nNote that, the assertion assert(c\u0026gt;0) will be shown as OK if the division by zero check is enabled. Because the assertion is after the division by zero error. The program is in an undefined state after the error, therefore, this result of OK should be ignored as Pinaka will already show that there is a division by zero error in the expression int c=a/b.\nIf you omit the flag for this check, it will correctly raise a warning for assertion violation.\npinaka filename.c\nOn the above program, this will correctly raise a warning of assertion assert(c\u0026gt;0) being violated.\nNote that along the same path, if there are multiple properties, Pinaka will correctly identify the status of the first property and ignore the rest, therefore, reporting the status of OK for other properties.\nFor example,\n#include \u0026lt;assert.h\u0026gt; int main() { int a,b; int c = a/b; int d = c/a; assert(c\u0026gt;0); return 0; } for the above program, when you use --div-by-zero-check it will show error only on int c=a/b and the rest will be ignored or shown as OK.\nPointer checks Pinaka supports certain checks with respect to pointers. All of the checks mentioned below are done when you provide --pointer-check flag to Pinaka.\nNull-pointer check #include \u0026lt;assert.h\u0026gt; #define NULL 0 int max (int *p, int *q) { int tmp1=*p; int tmp2=*q; if (tmp1 \u0026gt; tmp2) return tmp1; else return tmp2; } int main() { int *a=NULL; int b=5; int *c = \u0026amp;b; max(a,c); return 0; } Use the following command to run pointer-checks:\npinaka --pointer-check filename.c\nIt performs checks like NULL pointer dereference, invalid pointer usage, pointer use after deallocation of memory, pointer access outside the bounds, dangling pointer.\nUse-after-free check #include \u0026lt;assert.h\u0026gt; #define NULL 0 int max (int *p, int *q) { int tmp1=*p; int tmp2=*q; if (tmp1 \u0026gt; tmp2) return tmp1; else return tmp2; } int main() { int *a=NULL; int b=5; int *c = \u0026amp;b; // max(a,c); a=malloc(10*sizeof(int)); *a=10; free(a); *a=5; return 0; } Dangling pointer check example #include \u0026lt;assert.h\u0026gt; #define NULL 0 int a; int *q=\u0026amp;a; void foo () { int x; q=\u0026amp;x; } int main() { int *a=NULL; int b=5; int *c = \u0026amp;b; foo(); b = *q; return 0; } Outside dynamic object bounds check #include \u0026lt;assert.h\u0026gt; #define NULL 0 int a; int *q=\u0026amp;a; void foo () { int x; q=\u0026amp;x; } int main() { int *a=NULL; int b=5; int *c = \u0026amp;b; q=malloc(4*sizeof(int)); *(q+5)=5; return 0; } Outside object bounds #include \u0026lt;assert.h\u0026gt; #define NULL 0 int a; int *q=\u0026amp;a; void foo () { int x; q=\u0026amp;x; } int main() { int *a=NULL; int b=5; int *c = \u0026amp;b; int d[] = {1,2,3,4}; q=d; *(q+10)=10; return 0; } Memory leak detection Pinaka can check for memory leaks (allocated memory not freed during the lifetime of the program). Provide --memory-leak-check flag to perform such a check.\ninclude \u0026lt;assert.h\u0026gt; #define NULL 0 int a; int *q=\u0026amp;a; void foo () { int x; q=\u0026amp;x; } int main() { int *a=NULL; int b=5; int *c = \u0026amp;b; int d[] = {1,2,3,4}; q=malloc(4*sizeof(10)); *q=5; q=d; *(q+1)=10; return 0; } Use the command\npinaka --memory-leak-check filename.c\nNote Please note that Pinaka being a single-path symbolic-execution engine with early pruning will return as soon as the first violation of the given property is encountered. All the other properties will remain un-checked, and therefore, the status of the other properties, even if shown OK does not indicate that these properties hold.\nTermination check Pinaka unrolls any loop/recursion on-the-fly and therefore, the termination of the program can be checked by virtue of termination of execution of Pinaka itself. If Pinaka returns VERIFICATION SUCCESSFUL it also implies that the program terminates.\nIf the program is non-terminating (infinite loop or recursion), Pinaka will return if on some path there is a property violation, it will not terminate otherwise.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636990596,"objectID":"65262accd930a174738b33a5992c4b83","permalink":"https://sbjoshi.github.io/post/pinaka-usermanual/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/pinaka-usermanual/","section":"post","summary":"How to use Pinaka Pinaka is a single-path symbolic execution engine, built on top of CPROVER framework.\nIn the following text, we will show how to use Pinaka through various examples.","tags":null,"title":"Posts","type":"post"}]